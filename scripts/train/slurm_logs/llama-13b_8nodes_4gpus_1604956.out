+ module purge
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw purge
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash purge
Unloading profile/base
  ERROR: Module evaluation aborted
+ eval 'unset CPATH;
unset CMAKE_PREFIX_PATH;
unset CINECA_AI_LIB;
unset __MODULES_LMTAG;
unset OPENMPI_INC;
unset C_INCLUDE_PATH;
unset F77;
unset OPENMPI_LIB;
unset GCC_INCLUDE;
unset CUDA_INCLUDE;
MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
unset MPC_INC;
unset ZLIB_HOME;
unset MPIF77;
unset PKG_CONFIG_PATH;
unset OMPI_MCA_coll_hcoll_enable;
unset __MODULES_LMPREREQ;
unset CUDA_INC;
unset GCC_INC;
unset MPC_LIB;
unset CINECA_AI_INCLUDE;
unset F90;
unset MPFR_INCLUDE;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile; export __MODULES_LMALTNAME;
MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications; export MODULEPATH;
unset CUDA_LIB;
unset GCC_LIB;
unset GMP_HOME;
unset GMP_INC;
PATH=/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
unset MPIF90;
unset CUDA_HOME;
unset GCC_HOME;
unset MPFR_INC;
unset CC;
unset LD_LIBRARY_PATH;
unset GMP_LIB;
LOADEDMODULES=profile/base; export LOADEDMODULES;
unset MPFR_LIB;
unset OPENMPI_INCLUDE;
unset CXX;
unset ZLIB_INCLUDE;
unset MPFR_HOME;
unset OPENMPI_HOME;
unset MPICXX;
unset ZLIB_INC;
unset MPC_INCLUDE;
unset ZLIB_LIB;
unset MPC_HOME;
unset LIBRARY_PATH;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base; export _LMFILES_;
unset MPICC;
unset CPLUS_INCLUDE_PATH;
unset GMP_INCLUDE;
unset FC;
unset __MODULES_LMCONFLICT;
unset CINECA_AI_HOME;
unset CINECA_AI_INC;
test 0 = 1;'
++ unset CPATH
++ unset CMAKE_PREFIX_PATH
++ unset CINECA_AI_LIB
++ unset __MODULES_LMTAG
++ unset OPENMPI_INC
++ unset C_INCLUDE_PATH
++ unset F77
++ unset OPENMPI_LIB
++ unset GCC_INCLUDE
++ unset CUDA_INCLUDE
++ MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ unset MPC_INC
++ unset ZLIB_HOME
++ unset MPIF77
++ unset PKG_CONFIG_PATH
++ unset OMPI_MCA_coll_hcoll_enable
++ unset __MODULES_LMPREREQ
++ unset CUDA_INC
++ unset GCC_INC
++ unset MPC_LIB
++ unset CINECA_AI_INCLUDE
++ unset F90
++ unset MPFR_INCLUDE
++ __MODULES_LMALTNAME='profile/base&profile/default&profile'
++ export __MODULES_LMALTNAME
++ MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications
++ export MODULEPATH
++ unset CUDA_LIB
++ unset GCC_LIB
++ unset GMP_HOME
++ unset GMP_INC
++ PATH=/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ unset MPIF90
++ unset CUDA_HOME
++ unset GCC_HOME
++ unset MPFR_INC
++ unset CC
++ unset LD_LIBRARY_PATH
++ unset GMP_LIB
++ LOADEDMODULES=profile/base
++ export LOADEDMODULES
++ unset MPFR_LIB
++ unset OPENMPI_INCLUDE
++ unset CXX
++ unset ZLIB_INCLUDE
++ unset MPFR_HOME
++ unset OPENMPI_HOME
++ unset MPICXX
++ unset ZLIB_INC
++ unset MPC_INCLUDE
++ unset ZLIB_LIB
++ unset MPC_HOME
++ unset LIBRARY_PATH
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base
++ export _LMFILES_
++ unset MPICC
++ unset CPLUS_INCLUDE_PATH
++ unset GMP_INCLUDE
++ unset FC
++ unset __MODULES_LMCONFLICT
++ unset CINECA_AI_HOME
++ unset CINECA_AI_INC
++ test 0 = 1
+ _mlstatus=1
+ return 1
+ module load gcc
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load gcc
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load gcc
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
+ eval 'GCC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include; export GCC_INCLUDE;
MPC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb; export MPC_HOME;
LD_LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LD_LIBRARY_PATH;
CPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPATH;
MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
MPC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include; export MPC_INC;
LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LIBRARY_PATH;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0; export _LMFILES_;
GMP_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export GMP_LIB;
CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.; export CMAKE_PREFIX_PATH;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0; export LOADEDMODULES;
CXX=g++; export CXX;
MPFR_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib; export MPFR_LIB;
__MODULES_LMTAG=gmp/6.2.1\&auto-loaded:mpfr/4.1.0\&auto-loaded:mpc/1.2.1\&auto-loaded; export __MODULES_LMTAG;
PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig; export PKG_CONFIG_PATH;
MPFR_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc; export MPFR_HOME;
GCC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include; export GCC_INC;
MPC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib; export MPC_LIB;
__MODULES_LMPREREQ=mpfr/4.1.0\&gmp/6.2.1:mpc/1.2.1\&gmp/6.2.1\&mpfr/4.1.0:gcc/11.3.0\&gmp/6.2.1\&mpc/1.2.1\&mpfr/4.1.0; export __MODULES_LMPREREQ;
CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPLUS_INCLUDE_PATH;
F90=gfortran; export F90;
GMP_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export GMP_INCLUDE;
MPFR_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include; export MPFR_INCLUDE;
FC=gfortran; export FC;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile:gmp/6.2.1\&as\|gmp/default\&as\|gmp/latest:mpfr/4.1.0\&as\|mpfr/default\&as\|mpfr/latest:mpc/1.2.1\&as\|mpc/default\&as\|mpc/latest:gcc/11.3.0\&as\|gcc/default\&as\|gcc/latest; export __MODULES_LMALTNAME;
__MODULES_LMCONFLICT=gmp/6.2.1\&gmp:mpfr/4.1.0\&mpfr:mpc/1.2.1\&mpc:gcc/11.3.0\&gcc; export __MODULES_LMCONFLICT;
C_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export C_INCLUDE_PATH;
GCC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib; export GCC_LIB;
MPC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include; export MPC_INCLUDE;
F77=gfortran; export F77;
GMP_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22; export GMP_HOME;
GCC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza; export GCC_HOME;
PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
GMP_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export GMP_INC;
CC=gcc; export CC;
MPFR_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include; export MPFR_INC;
test 0;'
++ GCC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include
++ export GCC_INCLUDE
++ MPC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb
++ export MPC_HOME
++ LD_LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LD_LIBRARY_PATH
++ CPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPATH
++ MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ MPC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include
++ export MPC_INC
++ LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LIBRARY_PATH
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0
++ export _LMFILES_
++ GMP_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export GMP_LIB
++ CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.
++ export CMAKE_PREFIX_PATH
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0
++ export LOADEDMODULES
++ CXX=g++
++ export CXX
++ MPFR_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib
++ export MPFR_LIB
++ __MODULES_LMTAG='gmp/6.2.1&auto-loaded:mpfr/4.1.0&auto-loaded:mpc/1.2.1&auto-loaded'
++ export __MODULES_LMTAG
++ PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig
++ export PKG_CONFIG_PATH
++ MPFR_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc
++ export MPFR_HOME
++ GCC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include
++ export GCC_INC
++ MPC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib
++ export MPC_LIB
++ __MODULES_LMPREREQ='mpfr/4.1.0&gmp/6.2.1:mpc/1.2.1&gmp/6.2.1&mpfr/4.1.0:gcc/11.3.0&gmp/6.2.1&mpc/1.2.1&mpfr/4.1.0'
++ export __MODULES_LMPREREQ
++ CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPLUS_INCLUDE_PATH
++ F90=gfortran
++ export F90
++ GMP_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export GMP_INCLUDE
++ MPFR_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include
++ export MPFR_INCLUDE
++ FC=gfortran
++ export FC
++ __MODULES_LMALTNAME='profile/base&profile/default&profile:gmp/6.2.1&as|gmp/default&as|gmp/latest:mpfr/4.1.0&as|mpfr/default&as|mpfr/latest:mpc/1.2.1&as|mpc/default&as|mpc/latest:gcc/11.3.0&as|gcc/default&as|gcc/latest'
++ export __MODULES_LMALTNAME
++ __MODULES_LMCONFLICT='gmp/6.2.1&gmp:mpfr/4.1.0&mpfr:mpc/1.2.1&mpc:gcc/11.3.0&gcc'
++ export __MODULES_LMCONFLICT
++ C_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export C_INCLUDE_PATH
++ GCC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib
++ export GCC_LIB
++ MPC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include
++ export MPC_INCLUDE
++ F77=gfortran
++ export F77
++ GMP_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22
++ export GMP_HOME
++ GCC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza
++ export GCC_HOME
++ PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ GMP_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export GMP_INC
++ CC=gcc
++ export CC
++ MPFR_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include
++ export MPFR_INC
++ test 0
+ _mlstatus=0
+ return 0
+ module load cuda
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load cuda
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load cuda
+ eval 'LD_LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LD_LIBRARY_PATH;
CUDA_INCLUDE=/leonardo/prod/opt/compilers/cuda/11.8/none/include; export CUDA_INCLUDE;
CPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPATH;
LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LIBRARY_PATH;
MANPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8; export _LMFILES_;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8; export LOADEDMODULES;
CMAKE_PREFIX_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.; export CMAKE_PREFIX_PATH;
PKG_CONFIG_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig; export PKG_CONFIG_PATH;
CUDA_INC=/leonardo/prod/opt/compilers/cuda/11.8/none/include; export CUDA_INC;
CPLUS_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPLUS_INCLUDE_PATH;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile:gmp/6.2.1\&as\|gmp/default\&as\|gmp/latest:mpfr/4.1.0\&as\|mpfr/default\&as\|mpfr/latest:mpc/1.2.1\&as\|mpc/default\&as\|mpc/latest:gcc/11.3.0\&as\|gcc/default\&as\|gcc/latest:cuda/11.8\&as\|cuda/default\&as\|cuda/latest; export __MODULES_LMALTNAME;
__MODULES_LMCONFLICT=gmp/6.2.1\&gmp:mpfr/4.1.0\&mpfr:mpc/1.2.1\&mpc:gcc/11.3.0\&gcc:cuda/11.8\&cuda; export __MODULES_LMCONFLICT;
C_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export C_INCLUDE_PATH;
CUDA_LIB=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64; export CUDA_LIB;
CUDA_HOME=/leonardo/prod/opt/compilers/cuda/11.8/none; export CUDA_HOME;
PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
test 0;'
++ LD_LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LD_LIBRARY_PATH
++ CUDA_INCLUDE=/leonardo/prod/opt/compilers/cuda/11.8/none/include
++ export CUDA_INCLUDE
++ CPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPATH
++ LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LIBRARY_PATH
++ MANPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8
++ export _LMFILES_
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8
++ export LOADEDMODULES
++ CMAKE_PREFIX_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.
++ export CMAKE_PREFIX_PATH
++ PKG_CONFIG_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig
++ export PKG_CONFIG_PATH
++ CUDA_INC=/leonardo/prod/opt/compilers/cuda/11.8/none/include
++ export CUDA_INC
++ CPLUS_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPLUS_INCLUDE_PATH
++ __MODULES_LMALTNAME='profile/base&profile/default&profile:gmp/6.2.1&as|gmp/default&as|gmp/latest:mpfr/4.1.0&as|mpfr/default&as|mpfr/latest:mpc/1.2.1&as|mpc/default&as|mpc/latest:gcc/11.3.0&as|gcc/default&as|gcc/latest:cuda/11.8&as|cuda/default&as|cuda/latest'
++ export __MODULES_LMALTNAME
++ __MODULES_LMCONFLICT='gmp/6.2.1&gmp:mpfr/4.1.0&mpfr:mpc/1.2.1&mpc:gcc/11.3.0&gcc:cuda/11.8&cuda'
++ export __MODULES_LMCONFLICT
++ C_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export C_INCLUDE_PATH
++ CUDA_LIB=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64
++ export CUDA_LIB
++ CUDA_HOME=/leonardo/prod/opt/compilers/cuda/11.8/none
++ export CUDA_HOME
++ PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ test 0
+ _mlstatus=0
+ return 0
+ module load profile/deeplrn
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load profile/deeplrn
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load profile/deeplrn
+ eval '_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn; export _LMFILES_;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn; export LOADEDMODULES;
MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications:/leonardo/prod/opt/modulefiles/deeplrn/archive:/leonardo/prod/opt/modulefiles/deeplrn/dependencies:/leonardo/prod/opt/modulefiles/deeplrn/data:/leonardo/prod/opt/modulefiles/deeplrn/environment:/leonardo/prod/opt/modulefiles/deeplrn/libraries:/leonardo/prod/opt/modulefiles/deeplrn/tools:/leonardo/prod/opt/modulefiles/deeplrn/compilers:/leonardo/prod/opt/modulefiles/deeplrn/applications; export MODULEPATH;
test 0;'
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn
++ export _LMFILES_
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn
++ export LOADEDMODULES
++ MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications:/leonardo/prod/opt/modulefiles/deeplrn/archive:/leonardo/prod/opt/modulefiles/deeplrn/dependencies:/leonardo/prod/opt/modulefiles/deeplrn/data:/leonardo/prod/opt/modulefiles/deeplrn/environment:/leonardo/prod/opt/modulefiles/deeplrn/libraries:/leonardo/prod/opt/modulefiles/deeplrn/tools:/leonardo/prod/opt/modulefiles/deeplrn/compilers:/leonardo/prod/opt/modulefiles/deeplrn/applications
++ export MODULEPATH
++ test 0
+ _mlstatus=0
+ return 0
+ module load cineca-ai
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load cineca-ai
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load cineca-ai
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
+ eval 'LD_LIBRARY_PATH=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib64:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LD_LIBRARY_PATH;
CPATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/python-3.10.8-eauysn2mronkqqffs7r6bvftsdpsfm4b/include/python3.10:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPATH;
LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LIBRARY_PATH;
MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
ZLIB_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk; export ZLIB_HOME;
CINECA_AI_LIB=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib; export CINECA_AI_LIB;
MPIF77=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif77; export MPIF77;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn:/leonardo/prod/opt/modulefiles/base/libraries/zlib/1.2.13--gcc--11.3.0:/leonardo/prod/opt/modulefiles/base/libraries/openmpi/4.1.4--gcc--11.3.0-cuda-11.8:/leonardo/prod/opt/modulefiles/deeplrn/libraries/cineca-ai/3.0.0; export _LMFILES_;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn:zlib/1.2.13--gcc--11.3.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8:cineca-ai/3.0.0; export LOADEDMODULES;
CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/.:/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.; export CMAKE_PREFIX_PATH;
OPENMPI_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include; export OPENMPI_INCLUDE;
__MODULES_LMTAG=gmp/6.2.1\&auto-loaded:mpfr/4.1.0\&auto-loaded:mpc/1.2.1\&auto-loaded:zlib/1.2.13--gcc--11.3.0\&auto-loaded:openmpi/4.1.4--gcc--11.3.0-cuda-11.8\&auto-loaded; export __MODULES_LMTAG;
PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib/pkgconfig:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig; export PKG_CONFIG_PATH;
MPICC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpicc; export MPICC;
ZLIB_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include; export ZLIB_INCLUDE;
OMPI_MCA_coll_hcoll_enable=0; export OMPI_MCA_coll_hcoll_enable;
__MODULES_LMPREREQ=mpfr/4.1.0\&gmp/6.2.1:mpc/1.2.1\&gmp/6.2.1\&mpfr/4.1.0:gcc/11.3.0\&gmp/6.2.1\&mpc/1.2.1\&mpfr/4.1.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8\&zlib/1.2.13--gcc--11.3.0:cineca-ai/3.0.0\&cuda/11.8\&openmpi/4.1.4--gcc--11.3.0-cuda-11.8; export __MODULES_LMPREREQ;
CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPLUS_INCLUDE_PATH;
CINECA_AI_INCLUDE=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include; export CINECA_AI_INCLUDE;
OPENMPI_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn; export OPENMPI_HOME;
MPICXX=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpic++; export MPICXX;
OPENMPI_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include; export OPENMPI_INC;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile:gmp/6.2.1\&as\|gmp/default\&as\|gmp/latest:mpfr/4.1.0\&as\|mpfr/default\&as\|mpfr/latest:mpc/1.2.1\&as\|mpc/default\&as\|mpc/latest:gcc/11.3.0\&as\|gcc/default\&as\|gcc/latest:cuda/11.8\&as\|cuda/default\&as\|cuda/latest:zlib/1.2.13--gcc--11.3.0\&as\|zlib/default\&as\|zlib/latest:cineca-ai/3.0.0\&as\|cineca-ai/default\&as\|cineca-ai/latest; export __MODULES_LMALTNAME;
__MODULES_LMCONFLICT=gmp/6.2.1\&gmp:mpfr/4.1.0\&mpfr:mpc/1.2.1\&mpc:gcc/11.3.0\&gcc:cuda/11.8\&cuda:zlib/1.2.13--gcc--11.3.0\&zlib:openmpi/4.1.4--gcc--11.3.0-cuda-11.8\&openmpi:cineca-ai/3.0.0\&cineca-ai; export __MODULES_LMCONFLICT;
ZLIB_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include; export ZLIB_INC;
C_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export C_INCLUDE_PATH;
CINECA_AI_HOME=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none; export CINECA_AI_HOME;
CINECA_AI_INC=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include; export CINECA_AI_INC;
MPIF90=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif90; export MPIF90;
PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/bin:/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin:/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
OPENMPI_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib; export OPENMPI_LIB;
ZLIB_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib; export ZLIB_LIB;
test 0;'
++ LD_LIBRARY_PATH=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib64:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LD_LIBRARY_PATH
++ CPATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/python-3.10.8-eauysn2mronkqqffs7r6bvftsdpsfm4b/include/python3.10:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPATH
++ LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LIBRARY_PATH
++ MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ ZLIB_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk
++ export ZLIB_HOME
++ CINECA_AI_LIB=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib
++ export CINECA_AI_LIB
++ MPIF77=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif77
++ export MPIF77
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn:/leonardo/prod/opt/modulefiles/base/libraries/zlib/1.2.13--gcc--11.3.0:/leonardo/prod/opt/modulefiles/base/libraries/openmpi/4.1.4--gcc--11.3.0-cuda-11.8:/leonardo/prod/opt/modulefiles/deeplrn/libraries/cineca-ai/3.0.0
++ export _LMFILES_
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn:zlib/1.2.13--gcc--11.3.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8:cineca-ai/3.0.0
++ export LOADEDMODULES
++ CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/.:/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.
++ export CMAKE_PREFIX_PATH
++ OPENMPI_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include
++ export OPENMPI_INCLUDE
++ __MODULES_LMTAG='gmp/6.2.1&auto-loaded:mpfr/4.1.0&auto-loaded:mpc/1.2.1&auto-loaded:zlib/1.2.13--gcc--11.3.0&auto-loaded:openmpi/4.1.4--gcc--11.3.0-cuda-11.8&auto-loaded'
++ export __MODULES_LMTAG
++ PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib/pkgconfig:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig
++ export PKG_CONFIG_PATH
++ MPICC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpicc
++ export MPICC
++ ZLIB_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include
++ export ZLIB_INCLUDE
++ OMPI_MCA_coll_hcoll_enable=0
++ export OMPI_MCA_coll_hcoll_enable
++ __MODULES_LMPREREQ='mpfr/4.1.0&gmp/6.2.1:mpc/1.2.1&gmp/6.2.1&mpfr/4.1.0:gcc/11.3.0&gmp/6.2.1&mpc/1.2.1&mpfr/4.1.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8&zlib/1.2.13--gcc--11.3.0:cineca-ai/3.0.0&cuda/11.8&openmpi/4.1.4--gcc--11.3.0-cuda-11.8'
++ export __MODULES_LMPREREQ
++ CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPLUS_INCLUDE_PATH
++ CINECA_AI_INCLUDE=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include
++ export CINECA_AI_INCLUDE
++ OPENMPI_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn
++ export OPENMPI_HOME
++ MPICXX=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpic++
++ export MPICXX
++ OPENMPI_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include
++ export OPENMPI_INC
++ __MODULES_LMALTNAME='profile/base&profile/default&profile:gmp/6.2.1&as|gmp/default&as|gmp/latest:mpfr/4.1.0&as|mpfr/default&as|mpfr/latest:mpc/1.2.1&as|mpc/default&as|mpc/latest:gcc/11.3.0&as|gcc/default&as|gcc/latest:cuda/11.8&as|cuda/default&as|cuda/latest:zlib/1.2.13--gcc--11.3.0&as|zlib/default&as|zlib/latest:cineca-ai/3.0.0&as|cineca-ai/default&as|cineca-ai/latest'
++ export __MODULES_LMALTNAME
++ __MODULES_LMCONFLICT='gmp/6.2.1&gmp:mpfr/4.1.0&mpfr:mpc/1.2.1&mpc:gcc/11.3.0&gcc:cuda/11.8&cuda:zlib/1.2.13--gcc--11.3.0&zlib:openmpi/4.1.4--gcc--11.3.0-cuda-11.8&openmpi:cineca-ai/3.0.0&cineca-ai'
++ export __MODULES_LMCONFLICT
++ ZLIB_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include
++ export ZLIB_INC
++ C_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export C_INCLUDE_PATH
++ CINECA_AI_HOME=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none
++ export CINECA_AI_HOME
++ CINECA_AI_INC=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include
++ export CINECA_AI_INC
++ MPIF90=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif90
++ export MPIF90
++ PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/bin:/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin:/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ OPENMPI_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib
++ export OPENMPI_LIB
++ ZLIB_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib
++ export ZLIB_LIB
++ test 0
+ _mlstatus=0
+ return 0
+ source /leonardo_work/IscrC_GELATINO/gpuccett/llm-foundry_gpucce/venv/bin/activate
/var/spool/slurmd/job1604956/slurm_script: line 19: /leonardo_work/IscrC_GELATINO/gpuccett/llm-foundry_gpucce/venv/bin/activate: No such file or directory
+ srun /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
0: LOCAL_WORLD_SIZE=
0: LOCAL_WORLD_SIZE=4
0: NPROC=4
0: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
4: LOCAL_WORLD_SIZE=
4: LOCAL_WORLD_SIZE=4
4: NPROC=4
6: LOCAL_WORLD_SIZE=
4: WORLD SIZE=32
6: LOCAL_WORLD_SIZE=4
6: NPROC=4
6: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
3: LOCAL_WORLD_SIZE=
3: LOCAL_WORLD_SIZE=4
3: NPROC=4
3: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
2: LOCAL_WORLD_SIZE=
1: LOCAL_WORLD_SIZE=
1: LOCAL_WORLD_SIZE=4
1: NPROC=4
1: WORLD SIZE=32
2: LOCAL_WORLD_SIZE=4
2: NPROC=4
2: WORLD SIZE=32
7: LOCAL_WORLD_SIZE=
7: LOCAL_WORLD_SIZE=4
7: NPROC=4
7: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
5: LOCAL_WORLD_SIZE=
5: LOCAL_WORLD_SIZE=4
5: NPROC=4
5: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
0: MASTER_ADDR=lrdn0190
0: MASTER PORT=54321
0: PYTHONUNBUFFERED=1
0: BASE_RANK=0, NODEID=0
0: LOCAL_RANK=0
4: MASTER_ADDR=lrdn0190
4: MASTER PORT=54321
4: PYTHONUNBUFFERED=1
4: BASE_RANK=16, NODEID=4
4: LOCAL_RANK=0
6: MASTER_ADDR=lrdn0190
6: MASTER PORT=54321
6: PYTHONUNBUFFERED=1
6: BASE_RANK=24, NODEID=6
6: LOCAL_RANK=0
3: MASTER_ADDR=lrdn0190
3: MASTER PORT=54321
3: PYTHONUNBUFFERED=1
3: BASE_RANK=12, NODEID=3
3: LOCAL_RANK=0
1: MASTER_ADDR=lrdn0190
1: MASTER PORT=54321
1: PYTHONUNBUFFERED=1
1: BASE_RANK=4, NODEID=1
1: LOCAL_RANK=0
2: MASTER_ADDR=lrdn0190
2: MASTER PORT=54321
2: PYTHONUNBUFFERED=1
2: BASE_RANK=8, NODEID=2
2: LOCAL_RANK=0
7: MASTER_ADDR=lrdn0190
7: MASTER PORT=54321
7: PYTHONUNBUFFERED=1
7: BASE_RANK=28, NODEID=7
7: LOCAL_RANK=0
5: MASTER_ADDR=lrdn0190
5: MASTER PORT=54321
5: PYTHONUNBUFFERED=1
5: BASE_RANK=20, NODEID=5
5: LOCAL_RANK=0
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
ERROR:composer.cli.launcher:Rank 4 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 29 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 1 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 16 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 24 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 8 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 20 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 14 crashed with exit code 1.
Global rank 4 (PID 183219) exited with code 1
Global rank 5 (PID 183220) exited with code 1
----------Begin global rank 5 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 5 STDOUT----------
----------Begin global rank 5 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 5 STDERR----------
Global rank 6 (PID 183221) exited with code 1
----------Begin global rank 6 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 6 STDOUT----------
----------Begin global rank 6 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
Global rank 28 (PID 174968) exited with code 1
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
Global rank 0 (PID 192987) exited with code 1
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
Global rank 29 (PID 174969) exited with code 1
----------Begin global rank 29 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 29 STDOUT----------
----------Begin global rank 29 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
Global rank 1 (PID 192988) exited with code 1
----------Begin global rank 1 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 1 STDOUT----------
----------Begin global rank 1 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 6 STDERR----------
Global rank 7 (PID 183222) exited with code 1
----------Begin global rank 7 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 7 STDOUT----------
----------Begin global rank 7 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
Global rank 16 (PID 175485) exited with code 1
ERROR:composer.cli.launcher:Global rank 4 (PID 183219) exited with code 1
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
Global rank 17 (PID 175486) exited with code 1
----------Begin global rank 17 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 17 STDOUT----------
----------Begin global rank 17 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
Global rank 24 (PID 165256) exited with code 1
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
ERROR:composer.cli.launcher:Global rank 28 (PID 174968) exited with code 1
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
ERROR:composer.cli.launcher:Global rank 0 (PID 192987) exited with code 1
Global rank 25 (PID 165257) exited with code 1
----------Begin global rank 25 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 25 STDOUT----------
----------Begin global rank 25 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
ERROR:composer.cli.launcher:Global rank 16 (PID 175485) exited with code 1
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
ERROR:composer.cli.launcher:Global rank 24 (PID 165256) exited with code 1
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 29 STDERR----------
Global rank 30 (PID 174970) exited with code 1
----------Begin global rank 30 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 30 STDOUT----------
----------Begin global rank 30 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 1 STDERR----------
Global rank 2 (PID 192989) exited with code 1
----------Begin global rank 2 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 2 STDOUT----------
----------Begin global rank 2 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
Global rank 8 (PID 171184) exited with code 1
ERROR:composer.cli.launcher:Global rank 12 (PID 142384) exited with code 1
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
ERROR:composer.cli.launcher:Global rank 8 (PID 171184) exited with code 1
  110             validate_repo_id(arg_value)                            
   111                                                                     
ERROR:composer.cli.launcher:Global rank 20 (PID 157139) exited with code 1
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
Global rank 12 (PID 142384) exited with code 1
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
Global rank 9 (PID 171185) exited with code 1
----------Begin global rank 9 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 9 STDOUT----------
----------Begin global rank 9 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
Global rank 20 (PID 157139) exited with code 1
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 7 STDERR----------
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
Global rank 13 (PID 142385) exited with code 1
----------Begin global rank 13 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 13 STDOUT----------
----------Begin global rank 13 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 17 STDERR----------
Global rank 18 (PID 175487) exited with code 1
----------Begin global rank 18 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 18 STDOUT----------
----------Begin global rank 18 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
Global rank 21 (PID 157140) exited with code 1
----------Begin global rank 21 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 21 STDOUT----------
----------Begin global rank 21 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 25 STDERR----------
Global rank 26 (PID 165258) exited with code 1
----------Begin global rank 26 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 26 STDOUT----------
----------Begin global rank 26 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 30 STDERR----------
Global rank 31 (PID 174971) exited with code 1
----------Begin global rank 31 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 31 STDOUT----------
----------Begin global rank 31 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 2 STDERR----------
Global rank 3 (PID 192990) exited with code 1
----------Begin global rank 3 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 3 STDOUT----------
----------Begin global rank 3 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 13 STDERR----------
Global rank 14 (PID 142386) exited with code 1
----------Begin global rank 14 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 14 STDOUT----------
----------Begin global rank 14 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 9 STDERR----------
Global rank 10 (PID 171186) exited with code 1
----------Begin global rank 10 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 10 STDOUT----------
----------Begin global rank 10 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 21 STDERR----------
Global rank 22 (PID 157141) exited with code 1
----------Begin global rank 22 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 22 STDOUT----------
----------Begin global rank 22 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 18 STDERR----------
Global rank 19 (PID 175488) exited with code 1
----------Begin global rank 19 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 19 STDOUT----------
----------Begin global rank 19 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
  110             validate_repo_id(arg_value)                            
   111                                                                     
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 26 STDERR----------
Global rank 27 (PID 165259) exited with code 1
----------Begin global rank 27 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 27 STDOUT----------
----------Begin global rank 27 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
  110             validate_repo_id(arg_value)                            
   111                                                                     
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 31 STDERR----------
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 3 STDERR----------
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  110             validate_repo_id(arg_value)                            
   111                                                                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
  110             validate_repo_id(arg_value)                            
   111                                                                     
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 14 STDERR----------
Global rank 15 (PID 142387) exited with code 1
----------Begin global rank 15 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 15 STDOUT----------
----------Begin global rank 15 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 10 STDERR----------
Global rank 11 (PID 171187) exited with code 1
----------Begin global rank 11 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 11 STDOUT----------
----------Begin global rank 11 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 19 STDERR----------
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 22 STDERR----------
Global rank 23 (PID 157142) exited with code 1
----------Begin global rank 23 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 23 STDOUT----------
----------Begin global rank 23 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
 Traceback (most recent call last) 
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:588 in <module>                                                   
                                                                              
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   585    cfg = om.merge(yaml_cfg, cli_cfg)                                  
   586    om.resolve(cfg)                                                    
   587    assert isinstance(cfg, DictConfig)                                 
  588    main(cfg)                                                          
   589                                                                        
                                                                              
 /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai 
 n/train.py:440 in main                                                       
                                                                              
   437    # Build tokenizer                                                  
   438    tokenizer_name = tokenizer_config['name']                          
  110             validate_repo_id(arg_value)                            
   111                                                                     
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   439    tokenizer_kwargs = tokenizer_config.get('kwargs', {})              
  440    tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      
   441                                                                       
   442    # Scheduler                                                        
   443    scheduler_name: str = scheduler_config.pop('name')                 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b 
 uilders.py:170 in build_tokenizer                                            
                                                                              
   167    os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              
   168    os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
   169                                                                       
  170    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          
   171                                    **tokenizer_kwargs)      
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 27 STDERR----------
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
   172                                                                       
   173    # HuggingFace does not respect the model_max_length kwarg, and ove 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         
 from_pretrained                                                              
                                                                              
   683          return tokenizer_class.from_pretrained(pretrained_model_na 
   684                                                                      
   685       # Next, let's try to use the tokenizer_config file to get the  
  686       tokenizer_config = get_tokenizer_config(pretrained_model_name_ 
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
   687       if "_commit_hash" in tokenizer_config:                         
   688          kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  
   689       config_tokenizer_class = tokenizer_config.get("tokenizer_class 
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         
 get_tokenizer_config                                                         
                                                                              
   516       token = use_auth_token                                         
   517                                                                       
   518    commit_hash = kwargs.get("_commit_hash", None)                     
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
  519    resolved_config_file = cached_file(                                
   520       pretrained_model_name_or_path,                                 
   521       TOKENIZER_CONFIG_FILE,                                         
   522       cache_dir=cache_dir,                                           
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/transformers/utils/hub.py:429 in cached_file                 
                                                                              
    426    user_agent = http_user_agent(user_agent)                          
    427    try:                                                              
    428       # Load from URL or cache if already cached                    
  110             validate_repo_id(arg_value)                            
   111                                                                     
  110             validate_repo_id(arg_value)                            
   111                                                                     
   429       resolved_file = hf_hub_download(                              
    430          path_or_repo_id,                                          
    431          filename,                                                 
    432          subfolder=None if len(subfolder) == 0 else subfolder,     
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        
                                                                              
   107          kwargs.items(),  # Kwargs values                           
   108       ):                                                             
   109          if arg_name in ["repo_id", "from_id", "to_id"]:            
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
  110             validate_repo_id(arg_value)                            
   111                                                                     
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 15 STDERR----------
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 11 STDERR----------
   112          elif arg_name == "token" and arg_value is not None:        
   113             has_token = True                                       
                                                                              
 /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 
 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id 
                                                                              
   155       raise HFValidationError(f"Repo id must be a string, not {type( 
   156                                                                       
   157    if repo_id.count("/") > 1:                                         
  158       raise HFValidationError(                                       
   159          "Repo id must be in the form 'repo_name' or 'namespace/rep 
   160          f" '{repo_id}'. Use `repo_type` argument if needed."       
   161       )                                                              

HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 23 STDERR----------
srun: error: lrdn0235: task 2: Exited with exit code 1
srun: error: lrdn0297: task 7: Exited with exit code 1
srun: error: lrdn0248: task 4: Exited with exit code 1
srun: error: lrdn0278: task 6: Exited with exit code 1
srun: error: lrdn0193: task 1: Exited with exit code 1
srun: error: lrdn0240: task 3: Exited with exit code 1
srun: error: lrdn0190: task 0: Exited with exit code 1
srun: error: lrdn0265: task 5: Exited with exit code 1
