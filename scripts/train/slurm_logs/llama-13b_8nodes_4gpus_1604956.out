+ module purge
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw purge
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash purge
Unloading profile/base
  ERROR: Module evaluation aborted
+ eval 'unset CPATH;
unset CMAKE_PREFIX_PATH;
unset CINECA_AI_LIB;
unset __MODULES_LMTAG;
unset OPENMPI_INC;
unset C_INCLUDE_PATH;
unset F77;
unset OPENMPI_LIB;
unset GCC_INCLUDE;
unset CUDA_INCLUDE;
MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
unset MPC_INC;
unset ZLIB_HOME;
unset MPIF77;
unset PKG_CONFIG_PATH;
unset OMPI_MCA_coll_hcoll_enable;
unset __MODULES_LMPREREQ;
unset CUDA_INC;
unset GCC_INC;
unset MPC_LIB;
unset CINECA_AI_INCLUDE;
unset F90;
unset MPFR_INCLUDE;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile; export __MODULES_LMALTNAME;
MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications; export MODULEPATH;
unset CUDA_LIB;
unset GCC_LIB;
unset GMP_HOME;
unset GMP_INC;
PATH=/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
unset MPIF90;
unset CUDA_HOME;
unset GCC_HOME;
unset MPFR_INC;
unset CC;
unset LD_LIBRARY_PATH;
unset GMP_LIB;
LOADEDMODULES=profile/base; export LOADEDMODULES;
unset MPFR_LIB;
unset OPENMPI_INCLUDE;
unset CXX;
unset ZLIB_INCLUDE;
unset MPFR_HOME;
unset OPENMPI_HOME;
unset MPICXX;
unset ZLIB_INC;
unset MPC_INCLUDE;
unset ZLIB_LIB;
unset MPC_HOME;
unset LIBRARY_PATH;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base; export _LMFILES_;
unset MPICC;
unset CPLUS_INCLUDE_PATH;
unset GMP_INCLUDE;
unset FC;
unset __MODULES_LMCONFLICT;
unset CINECA_AI_HOME;
unset CINECA_AI_INC;
test 0 = 1;'
++ unset CPATH
++ unset CMAKE_PREFIX_PATH
++ unset CINECA_AI_LIB
++ unset __MODULES_LMTAG
++ unset OPENMPI_INC
++ unset C_INCLUDE_PATH
++ unset F77
++ unset OPENMPI_LIB
++ unset GCC_INCLUDE
++ unset CUDA_INCLUDE
++ MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ unset MPC_INC
++ unset ZLIB_HOME
++ unset MPIF77
++ unset PKG_CONFIG_PATH
++ unset OMPI_MCA_coll_hcoll_enable
++ unset __MODULES_LMPREREQ
++ unset CUDA_INC
++ unset GCC_INC
++ unset MPC_LIB
++ unset CINECA_AI_INCLUDE
++ unset F90
++ unset MPFR_INCLUDE
++ __MODULES_LMALTNAME='profile/base&profile/default&profile'
++ export __MODULES_LMALTNAME
++ MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications
++ export MODULEPATH
++ unset CUDA_LIB
++ unset GCC_LIB
++ unset GMP_HOME
++ unset GMP_INC
++ PATH=/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ unset MPIF90
++ unset CUDA_HOME
++ unset GCC_HOME
++ unset MPFR_INC
++ unset CC
++ unset LD_LIBRARY_PATH
++ unset GMP_LIB
++ LOADEDMODULES=profile/base
++ export LOADEDMODULES
++ unset MPFR_LIB
++ unset OPENMPI_INCLUDE
++ unset CXX
++ unset ZLIB_INCLUDE
++ unset MPFR_HOME
++ unset OPENMPI_HOME
++ unset MPICXX
++ unset ZLIB_INC
++ unset MPC_INCLUDE
++ unset ZLIB_LIB
++ unset MPC_HOME
++ unset LIBRARY_PATH
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base
++ export _LMFILES_
++ unset MPICC
++ unset CPLUS_INCLUDE_PATH
++ unset GMP_INCLUDE
++ unset FC
++ unset __MODULES_LMCONFLICT
++ unset CINECA_AI_HOME
++ unset CINECA_AI_INC
++ test 0 = 1
+ _mlstatus=1
+ return 1
+ module load gcc
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load gcc
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load gcc
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
+ eval 'GCC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include; export GCC_INCLUDE;
MPC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb; export MPC_HOME;
LD_LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LD_LIBRARY_PATH;
CPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPATH;
MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
MPC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include; export MPC_INC;
LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LIBRARY_PATH;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0; export _LMFILES_;
GMP_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export GMP_LIB;
CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.; export CMAKE_PREFIX_PATH;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0; export LOADEDMODULES;
CXX=g++; export CXX;
MPFR_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib; export MPFR_LIB;
__MODULES_LMTAG=gmp/6.2.1\&auto-loaded:mpfr/4.1.0\&auto-loaded:mpc/1.2.1\&auto-loaded; export __MODULES_LMTAG;
PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig; export PKG_CONFIG_PATH;
MPFR_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc; export MPFR_HOME;
GCC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include; export GCC_INC;
MPC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib; export MPC_LIB;
__MODULES_LMPREREQ=mpfr/4.1.0\&gmp/6.2.1:mpc/1.2.1\&gmp/6.2.1\&mpfr/4.1.0:gcc/11.3.0\&gmp/6.2.1\&mpc/1.2.1\&mpfr/4.1.0; export __MODULES_LMPREREQ;
CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPLUS_INCLUDE_PATH;
F90=gfortran; export F90;
GMP_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export GMP_INCLUDE;
MPFR_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include; export MPFR_INCLUDE;
FC=gfortran; export FC;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile:gmp/6.2.1\&as\|gmp/default\&as\|gmp/latest:mpfr/4.1.0\&as\|mpfr/default\&as\|mpfr/latest:mpc/1.2.1\&as\|mpc/default\&as\|mpc/latest:gcc/11.3.0\&as\|gcc/default\&as\|gcc/latest; export __MODULES_LMALTNAME;
__MODULES_LMCONFLICT=gmp/6.2.1\&gmp:mpfr/4.1.0\&mpfr:mpc/1.2.1\&mpc:gcc/11.3.0\&gcc; export __MODULES_LMCONFLICT;
C_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export C_INCLUDE_PATH;
GCC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib; export GCC_LIB;
MPC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include; export MPC_INCLUDE;
F77=gfortran; export F77;
GMP_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22; export GMP_HOME;
GCC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza; export GCC_HOME;
PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
GMP_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export GMP_INC;
CC=gcc; export CC;
MPFR_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include; export MPFR_INC;
test 0;'
++ GCC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include
++ export GCC_INCLUDE
++ MPC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb
++ export MPC_HOME
++ LD_LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LD_LIBRARY_PATH
++ CPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPATH
++ MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ MPC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include
++ export MPC_INC
++ LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LIBRARY_PATH
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0
++ export _LMFILES_
++ GMP_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export GMP_LIB
++ CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.
++ export CMAKE_PREFIX_PATH
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0
++ export LOADEDMODULES
++ CXX=g++
++ export CXX
++ MPFR_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib
++ export MPFR_LIB
++ __MODULES_LMTAG='gmp/6.2.1&auto-loaded:mpfr/4.1.0&auto-loaded:mpc/1.2.1&auto-loaded'
++ export __MODULES_LMTAG
++ PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig
++ export PKG_CONFIG_PATH
++ MPFR_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc
++ export MPFR_HOME
++ GCC_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include
++ export GCC_INC
++ MPC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib
++ export MPC_LIB
++ __MODULES_LMPREREQ='mpfr/4.1.0&gmp/6.2.1:mpc/1.2.1&gmp/6.2.1&mpfr/4.1.0:gcc/11.3.0&gmp/6.2.1&mpc/1.2.1&mpfr/4.1.0'
++ export __MODULES_LMPREREQ
++ CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPLUS_INCLUDE_PATH
++ F90=gfortran
++ export F90
++ GMP_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export GMP_INCLUDE
++ MPFR_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include
++ export MPFR_INCLUDE
++ FC=gfortran
++ export FC
++ __MODULES_LMALTNAME='profile/base&profile/default&profile:gmp/6.2.1&as|gmp/default&as|gmp/latest:mpfr/4.1.0&as|mpfr/default&as|mpfr/latest:mpc/1.2.1&as|mpc/default&as|mpc/latest:gcc/11.3.0&as|gcc/default&as|gcc/latest'
++ export __MODULES_LMALTNAME
++ __MODULES_LMCONFLICT='gmp/6.2.1&gmp:mpfr/4.1.0&mpfr:mpc/1.2.1&mpc:gcc/11.3.0&gcc'
++ export __MODULES_LMCONFLICT
++ C_INCLUDE_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export C_INCLUDE_PATH
++ GCC_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib
++ export GCC_LIB
++ MPC_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include
++ export MPC_INCLUDE
++ F77=gfortran
++ export F77
++ GMP_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22
++ export GMP_HOME
++ GCC_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza
++ export GCC_HOME
++ PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ GMP_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export GMP_INC
++ CC=gcc
++ export CC
++ MPFR_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include
++ export MPFR_INC
++ test 0
+ _mlstatus=0
+ return 0
+ module load cuda
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load cuda
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load cuda
+ eval 'LD_LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LD_LIBRARY_PATH;
CUDA_INCLUDE=/leonardo/prod/opt/compilers/cuda/11.8/none/include; export CUDA_INCLUDE;
CPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPATH;
LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LIBRARY_PATH;
MANPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8; export _LMFILES_;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8; export LOADEDMODULES;
CMAKE_PREFIX_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.; export CMAKE_PREFIX_PATH;
PKG_CONFIG_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig; export PKG_CONFIG_PATH;
CUDA_INC=/leonardo/prod/opt/compilers/cuda/11.8/none/include; export CUDA_INC;
CPLUS_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPLUS_INCLUDE_PATH;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile:gmp/6.2.1\&as\|gmp/default\&as\|gmp/latest:mpfr/4.1.0\&as\|mpfr/default\&as\|mpfr/latest:mpc/1.2.1\&as\|mpc/default\&as\|mpc/latest:gcc/11.3.0\&as\|gcc/default\&as\|gcc/latest:cuda/11.8\&as\|cuda/default\&as\|cuda/latest; export __MODULES_LMALTNAME;
__MODULES_LMCONFLICT=gmp/6.2.1\&gmp:mpfr/4.1.0\&mpfr:mpc/1.2.1\&mpc:gcc/11.3.0\&gcc:cuda/11.8\&cuda; export __MODULES_LMCONFLICT;
C_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export C_INCLUDE_PATH;
CUDA_LIB=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64; export CUDA_LIB;
CUDA_HOME=/leonardo/prod/opt/compilers/cuda/11.8/none; export CUDA_HOME;
PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
test 0;'
++ LD_LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LD_LIBRARY_PATH
++ CUDA_INCLUDE=/leonardo/prod/opt/compilers/cuda/11.8/none/include
++ export CUDA_INCLUDE
++ CPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPATH
++ LIBRARY_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LIBRARY_PATH
++ MANPATH=/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8
++ export _LMFILES_
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8
++ export LOADEDMODULES
++ CMAKE_PREFIX_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.
++ export CMAKE_PREFIX_PATH
++ PKG_CONFIG_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig
++ export PKG_CONFIG_PATH
++ CUDA_INC=/leonardo/prod/opt/compilers/cuda/11.8/none/include
++ export CUDA_INC
++ CPLUS_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPLUS_INCLUDE_PATH
++ __MODULES_LMALTNAME='profile/base&profile/default&profile:gmp/6.2.1&as|gmp/default&as|gmp/latest:mpfr/4.1.0&as|mpfr/default&as|mpfr/latest:mpc/1.2.1&as|mpc/default&as|mpc/latest:gcc/11.3.0&as|gcc/default&as|gcc/latest:cuda/11.8&as|cuda/default&as|cuda/latest'
++ export __MODULES_LMALTNAME
++ __MODULES_LMCONFLICT='gmp/6.2.1&gmp:mpfr/4.1.0&mpfr:mpc/1.2.1&mpc:gcc/11.3.0&gcc:cuda/11.8&cuda'
++ export __MODULES_LMCONFLICT
++ C_INCLUDE_PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export C_INCLUDE_PATH
++ CUDA_LIB=/leonardo/prod/opt/compilers/cuda/11.8/none/lib64
++ export CUDA_LIB
++ CUDA_HOME=/leonardo/prod/opt/compilers/cuda/11.8/none
++ export CUDA_HOME
++ PATH=/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ test 0
+ _mlstatus=0
+ return 0
+ module load profile/deeplrn
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load profile/deeplrn
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load profile/deeplrn
+ eval '_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn; export _LMFILES_;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn; export LOADEDMODULES;
MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications:/leonardo/prod/opt/modulefiles/deeplrn/archive:/leonardo/prod/opt/modulefiles/deeplrn/dependencies:/leonardo/prod/opt/modulefiles/deeplrn/data:/leonardo/prod/opt/modulefiles/deeplrn/environment:/leonardo/prod/opt/modulefiles/deeplrn/libraries:/leonardo/prod/opt/modulefiles/deeplrn/tools:/leonardo/prod/opt/modulefiles/deeplrn/compilers:/leonardo/prod/opt/modulefiles/deeplrn/applications; export MODULEPATH;
test 0;'
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn
++ export _LMFILES_
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn
++ export LOADEDMODULES
++ MODULEPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/modulefiles:/leonardo/prod/opt/modulefiles/profiles:/leonardo/prod/opt/modulefiles/base/archive:/leonardo/prod/opt/modulefiles/base/dependencies:/leonardo/prod/opt/modulefiles/base/data:/leonardo/prod/opt/modulefiles/base/environment:/leonardo/prod/opt/modulefiles/base/libraries:/leonardo/prod/opt/modulefiles/base/tools:/leonardo/prod/opt/modulefiles/base/compilers:/leonardo/prod/opt/modulefiles/base/applications:/leonardo/prod/opt/modulefiles/deeplrn/archive:/leonardo/prod/opt/modulefiles/deeplrn/dependencies:/leonardo/prod/opt/modulefiles/deeplrn/data:/leonardo/prod/opt/modulefiles/deeplrn/environment:/leonardo/prod/opt/modulefiles/deeplrn/libraries:/leonardo/prod/opt/modulefiles/deeplrn/tools:/leonardo/prod/opt/modulefiles/deeplrn/compilers:/leonardo/prod/opt/modulefiles/deeplrn/applications
++ export MODULEPATH
++ test 0
+ _mlstatus=0
+ return 0
+ module load cineca-ai
+ local _mlredir=1
+ '[' -n '' ']'
+ case " $@ " in
+ '[' 1 -eq 0 ']'
+ _module_raw load cineca-ai
++ /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/tcl-8.6.12-rdtv2pt4tnckxsgwtowik3sbexmxolve/bin/tclsh /leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/libexec/modulecmd.tcl bash load cineca-ai
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
+ eval 'LD_LIBRARY_PATH=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib64:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LD_LIBRARY_PATH;
CPATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/python-3.10.8-eauysn2mronkqqffs7r6bvftsdpsfm4b/include/python3.10:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPATH;
LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib; export LIBRARY_PATH;
MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man; export MANPATH;
ZLIB_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk; export ZLIB_HOME;
CINECA_AI_LIB=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib; export CINECA_AI_LIB;
MPIF77=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif77; export MPIF77;
_LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn:/leonardo/prod/opt/modulefiles/base/libraries/zlib/1.2.13--gcc--11.3.0:/leonardo/prod/opt/modulefiles/base/libraries/openmpi/4.1.4--gcc--11.3.0-cuda-11.8:/leonardo/prod/opt/modulefiles/deeplrn/libraries/cineca-ai/3.0.0; export _LMFILES_;
LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn:zlib/1.2.13--gcc--11.3.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8:cineca-ai/3.0.0; export LOADEDMODULES;
CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/.:/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.; export CMAKE_PREFIX_PATH;
OPENMPI_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include; export OPENMPI_INCLUDE;
__MODULES_LMTAG=gmp/6.2.1\&auto-loaded:mpfr/4.1.0\&auto-loaded:mpc/1.2.1\&auto-loaded:zlib/1.2.13--gcc--11.3.0\&auto-loaded:openmpi/4.1.4--gcc--11.3.0-cuda-11.8\&auto-loaded; export __MODULES_LMTAG;
PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib/pkgconfig:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig; export PKG_CONFIG_PATH;
MPICC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpicc; export MPICC;
ZLIB_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include; export ZLIB_INCLUDE;
OMPI_MCA_coll_hcoll_enable=0; export OMPI_MCA_coll_hcoll_enable;
__MODULES_LMPREREQ=mpfr/4.1.0\&gmp/6.2.1:mpc/1.2.1\&gmp/6.2.1\&mpfr/4.1.0:gcc/11.3.0\&gmp/6.2.1\&mpc/1.2.1\&mpfr/4.1.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8\&zlib/1.2.13--gcc--11.3.0:cineca-ai/3.0.0\&cuda/11.8\&openmpi/4.1.4--gcc--11.3.0-cuda-11.8; export __MODULES_LMPREREQ;
CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export CPLUS_INCLUDE_PATH;
CINECA_AI_INCLUDE=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include; export CINECA_AI_INCLUDE;
OPENMPI_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn; export OPENMPI_HOME;
MPICXX=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpic++; export MPICXX;
OPENMPI_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include; export OPENMPI_INC;
__MODULES_LMALTNAME=profile/base\&profile/default\&profile:gmp/6.2.1\&as\|gmp/default\&as\|gmp/latest:mpfr/4.1.0\&as\|mpfr/default\&as\|mpfr/latest:mpc/1.2.1\&as\|mpc/default\&as\|mpc/latest:gcc/11.3.0\&as\|gcc/default\&as\|gcc/latest:cuda/11.8\&as\|cuda/default\&as\|cuda/latest:zlib/1.2.13--gcc--11.3.0\&as\|zlib/default\&as\|zlib/latest:cineca-ai/3.0.0\&as\|cineca-ai/default\&as\|cineca-ai/latest; export __MODULES_LMALTNAME;
__MODULES_LMCONFLICT=gmp/6.2.1\&gmp:mpfr/4.1.0\&mpfr:mpc/1.2.1\&mpc:gcc/11.3.0\&gcc:cuda/11.8\&cuda:zlib/1.2.13--gcc--11.3.0\&zlib:openmpi/4.1.4--gcc--11.3.0-cuda-11.8\&openmpi:cineca-ai/3.0.0\&cineca-ai; export __MODULES_LMCONFLICT;
ZLIB_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include; export ZLIB_INC;
C_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include; export C_INCLUDE_PATH;
CINECA_AI_HOME=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none; export CINECA_AI_HOME;
CINECA_AI_INC=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include; export CINECA_AI_INC;
MPIF90=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif90; export MPIF90;
PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/bin:/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin:/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin; export PATH;
OPENMPI_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib; export OPENMPI_LIB;
ZLIB_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib; export ZLIB_LIB;
test 0;'
++ LD_LIBRARY_PATH=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib64:/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/compat:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LD_LIBRARY_PATH
++ CPATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/python-3.10.8-eauysn2mronkqqffs7r6bvftsdpsfm4b/include/python3.10:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPATH
++ LIBRARY_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib64:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib
++ export LIBRARY_PATH
++ MANPATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/share/man:/leonardo/prod/opt/compilers/cuda/11.8/none/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/share/man:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/share/man
++ export MANPATH
++ ZLIB_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk
++ export ZLIB_HOME
++ CINECA_AI_LIB=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/lib
++ export CINECA_AI_LIB
++ MPIF77=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif77
++ export MPIF77
++ _LMFILES_=/leonardo/prod/opt/modulefiles/profiles/profile/base:/leonardo/prod/opt/modulefiles/base/dependencies/gmp/6.2.1:/leonardo/prod/opt/modulefiles/base/dependencies/mpfr/4.1.0:/leonardo/prod/opt/modulefiles/base/dependencies/mpc/1.2.1:/leonardo/prod/opt/modulefiles/base/compilers/gcc/11.3.0:/leonardo/prod/opt/modulefiles/base/compilers/cuda/11.8:/leonardo/prod/opt/modulefiles/profiles/profile/deeplrn:/leonardo/prod/opt/modulefiles/base/libraries/zlib/1.2.13--gcc--11.3.0:/leonardo/prod/opt/modulefiles/base/libraries/openmpi/4.1.4--gcc--11.3.0-cuda-11.8:/leonardo/prod/opt/modulefiles/deeplrn/libraries/cineca-ai/3.0.0
++ export _LMFILES_
++ LOADEDMODULES=profile/base:gmp/6.2.1:mpfr/4.1.0:mpc/1.2.1:gcc/11.3.0:cuda/11.8:profile/deeplrn:zlib/1.2.13--gcc--11.3.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8:cineca-ai/3.0.0
++ export LOADEDMODULES
++ CMAKE_PREFIX_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/.:/leonardo/prod/opt/compilers/cuda/11.8/none/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/.:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/.
++ export CMAKE_PREFIX_PATH
++ OPENMPI_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include
++ export OPENMPI_INCLUDE
++ __MODULES_LMTAG='gmp/6.2.1&auto-loaded:mpfr/4.1.0&auto-loaded:mpc/1.2.1&auto-loaded:zlib/1.2.13--gcc--11.3.0&auto-loaded:openmpi/4.1.4--gcc--11.3.0-cuda-11.8&auto-loaded'
++ export __MODULES_LMTAG
++ PKG_CONFIG_PATH=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib/pkgconfig:/leonardo/prod/opt/compilers/cuda/11.8/none/lib64/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/lib/pkgconfig:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/lib/pkgconfig
++ export PKG_CONFIG_PATH
++ MPICC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpicc
++ export MPICC
++ ZLIB_INCLUDE=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include
++ export ZLIB_INCLUDE
++ OMPI_MCA_coll_hcoll_enable=0
++ export OMPI_MCA_coll_hcoll_enable
++ __MODULES_LMPREREQ='mpfr/4.1.0&gmp/6.2.1:mpc/1.2.1&gmp/6.2.1&mpfr/4.1.0:gcc/11.3.0&gmp/6.2.1&mpc/1.2.1&mpfr/4.1.0:openmpi/4.1.4--gcc--11.3.0-cuda-11.8&zlib/1.2.13--gcc--11.3.0:cineca-ai/3.0.0&cuda/11.8&openmpi/4.1.4--gcc--11.3.0-cuda-11.8'
++ export __MODULES_LMPREREQ
++ CPLUS_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export CPLUS_INCLUDE_PATH
++ CINECA_AI_INCLUDE=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include
++ export CINECA_AI_INCLUDE
++ OPENMPI_HOME=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn
++ export OPENMPI_HOME
++ MPICXX=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpic++
++ export MPICXX
++ OPENMPI_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include
++ export OPENMPI_INC
++ __MODULES_LMALTNAME='profile/base&profile/default&profile:gmp/6.2.1&as|gmp/default&as|gmp/latest:mpfr/4.1.0&as|mpfr/default&as|mpfr/latest:mpc/1.2.1&as|mpc/default&as|mpc/latest:gcc/11.3.0&as|gcc/default&as|gcc/latest:cuda/11.8&as|cuda/default&as|cuda/latest:zlib/1.2.13--gcc--11.3.0&as|zlib/default&as|zlib/latest:cineca-ai/3.0.0&as|cineca-ai/default&as|cineca-ai/latest'
++ export __MODULES_LMALTNAME
++ __MODULES_LMCONFLICT='gmp/6.2.1&gmp:mpfr/4.1.0&mpfr:mpc/1.2.1&mpc:gcc/11.3.0&gcc:cuda/11.8&cuda:zlib/1.2.13--gcc--11.3.0&zlib:openmpi/4.1.4--gcc--11.3.0-cuda-11.8&openmpi:cineca-ai/3.0.0&cineca-ai'
++ export __MODULES_LMCONFLICT
++ ZLIB_INC=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include
++ export ZLIB_INC
++ C_INCLUDE_PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/include:/leonardo/prod/opt/compilers/cuda/11.8/none/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpc-1.2.1-ndktnvrbytlmmfwre4fs5qbyzzso7fxb/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/mpfr-4.1.0-w4ofd2ll4ftu53yawc4d3sssrmeo2huc/include:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gmp-6.2.1-i7u7hnaqpscmi2x4ffsz6so6fftszm22/include
++ export C_INCLUDE_PATH
++ CINECA_AI_HOME=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none
++ export CINECA_AI_HOME
++ CINECA_AI_INC=/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/include
++ export CINECA_AI_INC
++ MPIF90=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin/mpif90
++ export MPIF90
++ PATH=/leonardo/prod/spack/03/ccsdeploy/spack_deploy/envs/cineca-ai-3/view/bin:/leonardo/prod/opt/libraries/cineca-ai/3.0.0/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/bin:/leonardo/prod/opt/compilers/cuda/11.8/none/samples/bin/x86_64/linux/release/:/leonardo/prod/opt/compilers/cuda/11.8/none/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/gcc-11.3.0-tm6phj7wkcw7cuy6gjixemkvh5x2mhza/bin:/leonardo_work/IscrC_GELATINO/gpuccett/models/model_venv/bin:/cineca/bin:/leonardo/home/userexternal/gpuccett/.vscode-server/bin/abd2f3db4bdb28f9e95536dfa84d8479f1eb312d/bin/remote-cli:/cineca/bin:/leonardo/home/userexternal/gpuccett/.local/bin:/leonardo/home/userexternal/gpuccett/bin:/cineca/bin:/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-8.5.0/environment-modules-5.2.0-rz47odw4phlhzhhbz7b65nv5s5othgmi/bin:/usr/local/bin:/usr/bin:/usr/local/sbin:/usr/sbin
++ export PATH
++ OPENMPI_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/openmpi-4.1.4-si334jjzldoxyl3jbbb3cf6legzshlqn/lib
++ export OPENMPI_LIB
++ ZLIB_LIB=/leonardo/prod/spack/03/install/0.19/linux-rhel8-icelake/gcc-11.3.0/zlib-1.2.13-noknejuy3uxqyxfetsmtnvb6zna3zaxk/lib
++ export ZLIB_LIB
++ test 0
+ _mlstatus=0
+ return 0
+ source /leonardo_work/IscrC_GELATINO/gpuccett/llm-foundry_gpucce/venv/bin/activate
/var/spool/slurmd/job1604956/slurm_script: line 19: /leonardo_work/IscrC_GELATINO/gpuccett/llm-foundry_gpucce/venv/bin/activate: No such file or directory
+ srun /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Unloading profile/base
  ERROR: Module evaluation aborted
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading gcc/11.3.0
  Loading requirement: gmp/6.2.1 mpfr/4.1.0 mpc/1.2.1
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
0: LOCAL_WORLD_SIZE=
0: LOCAL_WORLD_SIZE=4
0: NPROC=4
0: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
4: LOCAL_WORLD_SIZE=
4: LOCAL_WORLD_SIZE=4
4: NPROC=4
6: LOCAL_WORLD_SIZE=
4: WORLD SIZE=32
6: LOCAL_WORLD_SIZE=4
6: NPROC=4
6: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
3: LOCAL_WORLD_SIZE=
3: LOCAL_WORLD_SIZE=4
3: NPROC=4
3: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
2: LOCAL_WORLD_SIZE=
1: LOCAL_WORLD_SIZE=
1: LOCAL_WORLD_SIZE=4
1: NPROC=4
1: WORLD SIZE=32
2: LOCAL_WORLD_SIZE=4
2: NPROC=4
2: WORLD SIZE=32
7: LOCAL_WORLD_SIZE=
7: LOCAL_WORLD_SIZE=4
7: NPROC=4
7: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
Loading cineca-ai/3.0.0
  Loading requirement: zlib/1.2.13--gcc--11.3.0
    openmpi/4.1.4--gcc--11.3.0-cuda-11.8
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
5: LOCAL_WORLD_SIZE=
5: LOCAL_WORLD_SIZE=4
5: NPROC=4
5: WORLD SIZE=32
/leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/fine_tuning_commands/test_salloc_llama-70b-finetune_multinode_8nodes_4gpus.sbatch: line 26: /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/train/sbatch_scripts: Is a directory
0: MASTER_ADDR=lrdn0190
0: MASTER PORT=54321
0: PYTHONUNBUFFERED=1
0: BASE_RANK=0, NODEID=0
0: LOCAL_RANK=0
4: MASTER_ADDR=lrdn0190
4: MASTER PORT=54321
4: PYTHONUNBUFFERED=1
4: BASE_RANK=16, NODEID=4
4: LOCAL_RANK=0
6: MASTER_ADDR=lrdn0190
6: MASTER PORT=54321
6: PYTHONUNBUFFERED=1
6: BASE_RANK=24, NODEID=6
6: LOCAL_RANK=0
3: MASTER_ADDR=lrdn0190
3: MASTER PORT=54321
3: PYTHONUNBUFFERED=1
3: BASE_RANK=12, NODEID=3
3: LOCAL_RANK=0
1: MASTER_ADDR=lrdn0190
1: MASTER PORT=54321
1: PYTHONUNBUFFERED=1
1: BASE_RANK=4, NODEID=1
1: LOCAL_RANK=0
2: MASTER_ADDR=lrdn0190
2: MASTER PORT=54321
2: PYTHONUNBUFFERED=1
2: BASE_RANK=8, NODEID=2
2: LOCAL_RANK=0
7: MASTER_ADDR=lrdn0190
7: MASTER PORT=54321
7: PYTHONUNBUFFERED=1
7: BASE_RANK=28, NODEID=7
7: LOCAL_RANK=0
5: MASTER_ADDR=lrdn0190
5: MASTER PORT=54321
5: PYTHONUNBUFFERED=1
5: BASE_RANK=20, NODEID=5
5: LOCAL_RANK=0
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.
ERROR:composer.cli.launcher:Rank 4 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 29 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 1 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 16 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 24 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 8 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 20 crashed with exit code 1.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
Waiting up to 30 seconds for all training processes to terminate. Press Ctrl-C to exit immediately.
ERROR:composer.cli.launcher:Rank 14 crashed with exit code 1.
Global rank 4 (PID 183219) exited with code 1
Global rank 5 (PID 183220) exited with code 1
----------Begin global rank 5 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 5 STDOUT----------
----------Begin global rank 5 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 5 STDERR----------
Global rank 6 (PID 183221) exited with code 1
----------Begin global rank 6 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 6 STDOUT----------
----------Begin global rank 6 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
Global rank 28 (PID 174968) exited with code 1
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
Global rank 0 (PID 192987) exited with code 1
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
Global rank 29 (PID 174969) exited with code 1
----------Begin global rank 29 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 29 STDOUT----------
----------Begin global rank 29 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
Global rank 1 (PID 192988) exited with code 1
----------Begin global rank 1 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 1 STDOUT----------
----------Begin global rank 1 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 6 STDERR----------
Global rank 7 (PID 183222) exited with code 1
----------Begin global rank 7 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 7 STDOUT----------
----------Begin global rank 7 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
Global rank 16 (PID 175485) exited with code 1
ERROR:composer.cli.launcher:Global rank 4 (PID 183219) exited with code 1
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
Global rank 17 (PID 175486) exited with code 1
----------Begin global rank 17 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 17 STDOUT----------
----------Begin global rank 17 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
Global rank 24 (PID 165256) exited with code 1
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
ERROR:composer.cli.launcher:Global rank 28 (PID 174968) exited with code 1
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
ERROR:composer.cli.launcher:Global rank 0 (PID 192987) exited with code 1
Global rank 25 (PID 165257) exited with code 1
----------Begin global rank 25 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 25 STDOUT----------
----------Begin global rank 25 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
ERROR:composer.cli.launcher:Global rank 16 (PID 175485) exited with code 1
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
ERROR:composer.cli.launcher:Global rank 24 (PID 165256) exited with code 1
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 29 STDERR----------
Global rank 30 (PID 174970) exited with code 1
----------Begin global rank 30 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 30 STDOUT----------
----------Begin global rank 30 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 1 STDERR----------
Global rank 2 (PID 192989) exited with code 1
----------Begin global rank 2 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 2 STDOUT----------
----------Begin global rank 2 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
Global rank 8 (PID 171184) exited with code 1
ERROR:composer.cli.launcher:Global rank 12 (PID 142384) exited with code 1
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
ERROR:composer.cli.launcher:Global rank 8 (PID 171184) exited with code 1
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
ERROR:composer.cli.launcher:Global rank 20 (PID 157139) exited with code 1
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
Global rank 12 (PID 142384) exited with code 1
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
Global rank 9 (PID 171185) exited with code 1
----------Begin global rank 9 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 9 STDOUT----------
----------Begin global rank 9 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
Global rank 20 (PID 157139) exited with code 1
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 7 STDERR----------
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
Global rank 13 (PID 142385) exited with code 1
----------Begin global rank 13 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 13 STDOUT----------
----------Begin global rank 13 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 17 STDERR----------
Global rank 18 (PID 175487) exited with code 1
----------Begin global rank 18 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 18 STDOUT----------
----------Begin global rank 18 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
Global rank 21 (PID 157140) exited with code 1
----------Begin global rank 21 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 21 STDOUT----------
----------Begin global rank 21 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 25 STDERR----------
Global rank 26 (PID 165258) exited with code 1
----------Begin global rank 26 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 26 STDOUT----------
----------Begin global rank 26 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 30 STDERR----------
Global rank 31 (PID 174971) exited with code 1
----------Begin global rank 31 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 31 STDOUT----------
----------Begin global rank 31 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 2 STDERR----------
Global rank 3 (PID 192990) exited with code 1
----------Begin global rank 3 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 3 STDOUT----------
----------Begin global rank 3 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 13 STDERR----------
Global rank 14 (PID 142386) exited with code 1
----------Begin global rank 14 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 14 STDOUT----------
----------Begin global rank 14 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 9 STDERR----------
Global rank 10 (PID 171186) exited with code 1
----------Begin global rank 10 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 10 STDOUT----------
----------Begin global rank 10 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 21 STDERR----------
Global rank 22 (PID 157141) exited with code 1
----------Begin global rank 22 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 22 STDOUT----------
----------Begin global rank 22 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 18 STDERR----------
Global rank 19 (PID 175488) exited with code 1
----------Begin global rank 19 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 19 STDOUT----------
----------Begin global rank 19 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 26 STDERR----------
Global rank 27 (PID 165259) exited with code 1
----------Begin global rank 27 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 27 STDOUT----------
----------Begin global rank 27 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 31 STDERR----------
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 3 STDERR----------
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 14 STDERR----------
Global rank 15 (PID 142387) exited with code 1
----------Begin global rank 15 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 15 STDOUT----------
----------Begin global rank 15 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 10 STDERR----------
Global rank 11 (PID 171187) exited with code 1
----------Begin global rank 11 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 11 STDOUT----------
----------Begin global rank 11 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 19 STDERR----------
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 22 STDERR----------
Global rank 23 (PID 157142) exited with code 1
----------Begin global rank 23 STDOUT----------
As run_name, save_folder, and save_latest_filename are set,                 changing autoresume default to True...

----------End global rank 23 STDOUT----------
----------Begin global rank 23 STDERR----------
device_microbatch_size > device_batch_size, will be reduced from 8 -> 4.
╭───────────────────── Traceback (most recent call last) ──────────────────────╮
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:588 in <module>                                                   │
│                                                                              │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   585 │   cfg = om.merge(yaml_cfg, cli_cfg)                                  │
│   586 │   om.resolve(cfg)                                                    │
│   587 │   assert isinstance(cfg, DictConfig)                                 │
│ ❱ 588 │   main(cfg)                                                          │
│   589                                                                        │
│                                                                              │
│ /leonardo_work/IscrC_GELATINO/gpuccett/Repos/llm-foundry_gpucce/scripts/trai │
│ n/train.py:440 in main                                                       │
│                                                                              │
│   437 │   # Build tokenizer                                                  │
│   438 │   tokenizer_name = tokenizer_config['name']                          │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   439 │   tokenizer_kwargs = tokenizer_config.get('kwargs', {})              │
│ ❱ 440 │   tokenizer = build_tokenizer(tokenizer_name, tokenizer_kwargs)      │
│   441 │                                                                      │
│   442 │   # Scheduler                                                        │
│   443 │   scheduler_name: str = scheduler_config.pop('name')                 │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/llmfoundry/utils/b │
│ uilders.py:170 in build_tokenizer                                            │
│                                                                              │
│   167 │   os.environ['TRANSFORMERS_NO_ADVISORY_WARNINGS'] = '1'              │
│   168 │   os.environ['TOKENIZERS_PARALLELISM'] = 'false'                     │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│   169 │                                                                      │
│ ❱ 170 │   tokenizer = AutoTokenizer.from_pretrained(tokenizer_name,          │
│   171 │   │   │   │   │   │   │   │   │   │   │     **tokenizer_kwargs)      │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 27 STDERR----------
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│   172 │                                                                      │
│   173 │   # HuggingFace does not respect the model_max_length kwarg, and ove │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:686 in         │
│ from_pretrained                                                              │
│                                                                              │
│   683 │   │   │   return tokenizer_class.from_pretrained(pretrained_model_na │
│   684 │   │                                                                  │
│   685 │   │   # Next, let's try to use the tokenizer_config file to get the  │
│ ❱ 686 │   │   tokenizer_config = get_tokenizer_config(pretrained_model_name_ │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│   687 │   │   if "_commit_hash" in tokenizer_config:                         │
│   688 │   │   │   kwargs["_commit_hash"] = tokenizer_config["_commit_hash"]  │
│   689 │   │   config_tokenizer_class = tokenizer_config.get("tokenizer_class │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/models/auto/tokenization_auto.py:519 in         │
│ get_tokenizer_config                                                         │
│                                                                              │
│   516 │   │   token = use_auth_token                                         │
│   517 │                                                                      │
│   518 │   commit_hash = kwargs.get("_commit_hash", None)                     │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│ ❱ 519 │   resolved_config_file = cached_file(                                │
│   520 │   │   pretrained_model_name_or_path,                                 │
│   521 │   │   TOKENIZER_CONFIG_FILE,                                         │
│   522 │   │   cache_dir=cache_dir,                                           │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/transformers/utils/hub.py:429 in cached_file                 │
│                                                                              │
│    426 │   user_agent = http_user_agent(user_agent)                          │
│    427 │   try:                                                              │
│    428 │   │   # Load from URL or cache if already cached                    │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│ ❱  429 │   │   resolved_file = hf_hub_download(                              │
│    430 │   │   │   path_or_repo_id,                                          │
│    431 │   │   │   filename,                                                 │
│    432 │   │   │   subfolder=None if len(subfolder) == 0 else subfolder,     │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:110 in _inner_fn        │
│                                                                              │
│   107 │   │   │   kwargs.items(),  # Kwargs values                           │
│   108 │   │   ):                                                             │
│   109 │   │   │   if arg_name in ["repo_id", "from_id", "to_id"]:            │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│ ❱ 110 │   │   │   │   validate_repo_id(arg_value)                            │
│   111 │   │   │                                                              │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 15 STDERR----------
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 11 STDERR----------
│   112 │   │   │   elif arg_name == "token" and arg_value is not None:        │
│   113 │   │   │   │   has_token = True                                       │
│                                                                              │
│ /leonardo_scratch/large/userexternal/gpuccett/llm-foundry/venv/lib/python3.1 │
│ 0/site-packages/huggingface_hub/utils/_validators.py:158 in validate_repo_id │
│                                                                              │
│   155 │   │   raise HFValidationError(f"Repo id must be a string, not {type( │
│   156 │                                                                      │
│   157 │   if repo_id.count("/") > 1:                                         │
│ ❱ 158 │   │   raise HFValidationError(                                       │
│   159 │   │   │   "Repo id must be in the form 'repo_name' or 'namespace/rep │
│   160 │   │   │   f" '{repo_id}'. Use `repo_type` argument if needed."       │
│   161 │   │   )                                                              │
╰──────────────────────────────────────────────────────────────────────────────╯
HFValidationError: Repo id must be in the form 'repo_name' or 
'namespace/repo_name': 
'/leonardo_work/IscrC_GELATINO/gpuccett/models/llama-2-70b'. Use `repo_type` 
argument if needed.

----------End global rank 23 STDERR----------
srun: error: lrdn0235: task 2: Exited with exit code 1
srun: error: lrdn0297: task 7: Exited with exit code 1
srun: error: lrdn0248: task 4: Exited with exit code 1
srun: error: lrdn0278: task 6: Exited with exit code 1
srun: error: lrdn0193: task 1: Exited with exit code 1
srun: error: lrdn0240: task 3: Exited with exit code 1
srun: error: lrdn0190: task 0: Exited with exit code 1
srun: error: lrdn0265: task 5: Exited with exit code 1
